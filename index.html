<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
</head>


<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
		<a class="navbar-brand" href="#">Luxin Zhang</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
	  		<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Education">Education</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Research">Research</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item dropdown">
					<a class="nav-link dropdown-toggle" href="" id="projects" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects</a>
					<div class="dropdown-menu dropdown-menu-right" aria-labelledby="projects">
						<a class="dropdown-item" href="#Project1">Gesture Recognition</a>
						<a class="dropdown-item" href="#Project2">Text and Image Classification</a>
						<a class="dropdown-item" href="#Project3">Face Detection and Recognition</a>
						<a class="dropdown-item" href="#Project4">Data Visualization</a>
						<a class="dropdown-item" href="#Project5">Robots Control</a>
						<a class="dropdown-item" href="#Project6">Image Processing</a>
						<a class="dropdown-item" href="#Project7">Mine Sweeper</a>
					</div>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Honors">Honors</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 20px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 0px; padding-top: 62px">
				<img class="img-responsive img-rounded" src="img/Luxin.jpeg" alt="" style="max-width: 260px">
			</div>
			<div class="col-md-9">
			<br><br>

			<p>Hi, I'm Luxin Zhang!</br>
			I am a senior undergraduate student at 
			<b><a target="_blank" href="http://www.cis.pku.edu.cn/english_index.htm">Department of Intelligence Science</a></b>, 
			<b><a target="_blank" href="http://eecs.pku.edu.cn/EN/">School of EECS</a></b>, 
			<b><a target="_blank" href="http://english.pku.edu.cn/">Peking University</a></b>.<br>

			I was a visiting student at
			<b><a target="_blank" href="https://www.cs.utexas.edu/">Department of Computer Science</a></b>, 
			<b><a target="_blank" href="https://www.utexas.edu/">The University of Texas at Austin</a></b>, 
			instructed by Professor 
			<b><a target="_blank" href="http://www.cs.utexas.edu/~dana/">Dana Ballard</a></b> 
			during July and August 2017.<br>

			I was an intern in 
			<b><a target="_blank" href="https://www.microsoft.com/en-us/research/group/bdm/">Big Data Mining Group</a></b> at 
			<b><a target="_blank" href="http://www.msra.cn/">Microsoft Research Asia</a></b>, supervised by Dr. 
			<b><a target="_blank" href="https://www.microsoft.com/en-us/research/people/borjekar/">Borje Karlsson</a></b> 
			during September 2017 and Janurary 2018.<br>

			I am interested in Machine Learning, Computer Vision and Artificial Intelligence.<br>
			<br>
			<a href="static/Luxin Zhang.pdf">CV</a> | <a href="https://github.com/lucinezhang">Github</a> | <a href="https://www.linkedin.com/in/luxin-zhang-0264a411a/">LinkedIn</a></p>
			</p>

			</div>
		</div>
	</div><br><br>

	
	<!-- Education -->
	<div class="container">
		<h3 id="Education" style="padding-top: 80px; margin-top: -80px;">Educations</h3><hr>
		<table class="imgtable"><tr><td>
			<img src="img/pku.png" alt="Peking" width="100px" height="100px" />&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td align="left">
				<p><i>Sept. 2014 - Jul. 2018 (Expected)</i> <br>
				<a target="_blank" href="http://www.cis.pku.edu.cn/english_index.htm">Department of Computer Intelligence Science</a>, 
				<b><a target="_blank" href="http://english.pku.edu.cn/">Peking University</a></b><br>
				Bachelor of Science <br>
				<b>Overall GPA: 3.56/4.0 &nbsp; Junior GPA: 3.70/4.0</b>
				</p>
		</td></tr></table>

		<table class="imgtable"><tr><td>
			<img src="img/utaustin.png" alt="UT-Austin" width="100px" height="100px" />&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td align="left">
				<p><i>Jul. 2017 - Sept. 2017</i> <br>
				<a target="_blank" href="https://www.cs.utexas.edu/">School of Computer Science</a>, 
				<b><a target="_blank" href="https://www.utexas.edu/">The University of Texas at Austin</a></b> <br>
				<a target="_blank" href="http://www.cs.utexas.edu/~dana/vrlab/">Vision, Cognition, and Action VR Lab</a> <br>
				Research Assistant <br>
				Supervised by Professor <b>Dana Ballard</b>.
				</p>
		</td></tr></table>
		<hr>

		<br>
	</div><br>

	
	<!-- Research -->
	<div class="container">
		<h3 id="Research" style="padding-top: 80px; margin-top: -80px;">Research</h3><hr>
			<h4 style="padding-top: 80px; margin-top: -80px;">Modeling human attention for deep imitation learning</h4>
				<p>Time: <i>Jul. 2017 - Present</i></p>
				<img src="img/seaquest.png" height="180px">
				<img src="img/breakout.png" height="180px">
				<img src="img/gaze_network.png" height="180px">
				<img src="img/action_network.png" height="150px">
				<br>
				<p>When an intelligent agent learns to imitate human visuomotor behaviors, it may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agent's performance. With this motivation, we collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another deep neural network to predict human actions (the policy network). Including the gaze predictions from the gaze network in the policy network significantly improves the action prediction accuracy. We conclude that it is feasible to learn human attention in the given visuomotor tasks, and that combining the learned attention model with imitation learning yields promising results.<br><br>

				My work was to model visual attention from human eye movement data using a deep learning approach. I helped conduct psychophysical experiments that collect high-precision human eye tracking data when playing video games, and designed a three-channel convolution-deconvolution deep neural network, that simultaneously takes game image frames, motion information (optical flow), and image saliency information to predict where the human would allocate visual attention when playing the game. Eventually I obtained a deep neural network model that can predict human visual attention with high precision. Now we are experimenting on using this visual attention model to facilitate the learning process of deep reinforcement learning and deep imitation learning algorithms.
				</p><hr>
				
			<h4 style="padding-top: 80px; margin-top: -80px;">Text effects transfer</h4>
				<p>Time: <i>Mar. 2017 - Jun. 2017</i></p>
				<img src="img/text_transfer.png" height="180px">
				<br>
				<p>Text effects transfer is a pretty novel research area. We studied the problem of transferring the text styles from source text image to target text image, that is, given a source stylized image S' and the target text image T, then automatically generates the target stylized image T' with the special e ects as in S'. I tried different image segmentation methods to create a text mask for the stylized image S', like KNN clustering based on pixels' feature vectors and level set segmentation based on shape priors.
				</p><hr>

			<h4 style="padding-top: 80px; margin-top: -80px;">Cultural heritage protection based on virtual reality</h4>
				<p>Time: <i>Mar. 2016 - Jun. 2016</i></p>
				<br>
				<p>I participated in an interdisciplinary project between computer vision and archaeological conservation, in which we attempted to repair the faces of the Buddha of Longmen Grottoes using archived photos and display them on a virtual reality (VR) system. I was responsible for designing the VR user interface based on gesture recognition to enable the visitors to instruct the system to display the images, show text descriptions, or start the voice guide. I used Hidden Markov Model to implement a three-class gesture recognition, achieving an accuracy of 96% which met the requirements of the project.
				</p><hr>
		
		<br>
	</div><br>
	
	
	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">Publications</h3><hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="img/seaquest.png" alt="" height="180px">
			</div>
			<div class="col-md-9">
			<b><font color="black">Learning Attention Model from Human for Visuomotor Tasks</font></b><br>
			<a target="_blank" href="http://lucinezhang.github.io"><u><font color="black">L. Zhang</font></u></a>,
			<a target="_blank" href="http://www.cs.utexas.edu/~zharucs/">R. Zhang</a>, 
			Z. Liu, 
			<a target="_blank" href="https://liberalarts.utexas.edu/cps/faculty/profile.php?id=mmh739">M. Hayhoe</a> 
			and 
			<a target="_blank" href="http://www.cs.utexas.edu/~dana/">D. Ballard</a>
			<br>
			<i><a target="_blank" href="https://aaai.org/Conferences/AAAI-18/aaai18studentcall/">AAAI'18 Student Abstract and Poster Program</a></i>, New Orleans, Louisiana, USA, Feb. 2018<br>
			<a target="_blank" href="static/AAAI-18.pdf"> <small>[PDF]</small></a>
			<a target="_blank" href="https://www.youtube.com/watch?v=-zTX9VFSFME"> <small>[Video]</small></a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="img/gaze_network.png" alt="">
				<img class="img-fluid img-rounded" src="img/action_network.png" alt="">
			</div>
			<div class="col-md-9">
			<b><font color="black">Visual Attention Guided Deep Imitation Learning</font></b>
			<br>
			<a target="_blank" href="http://www.cs.utexas.edu/~zharucs/">R. Zhang</a>, 
			Z. Liu, 
			<a target="_blank" href="http://lucinezhang.github.io"><u><font color="black">L. Zhang</font></u></a>,
			K. Muller, 
			<a target="_blank" href="https://liberalarts.utexas.edu/cps/faculty/profile.php?id=mmh739">M. Hayhoe</a> 
			and 
			<a target="_blank" href="http://www.cs.utexas.edu/~dana/">D. Ballard</a>
			<br>
			spotlight paper, 
			<i><a target="_blank" href="https://sites.google.com/view/ciai2017/home">NIPS'17 Cognitively Informed Artificial Intelligence Workshop</a></i>, Long Beach, California, USA, Dec. 2017<br>
			<a target="_blank" href="static/2017_NIPS.pdf"> <small>[PDF]</small></a>
			</div>
		</div><hr>
		
		<br>
	</div><br>
	
	
	<!-- Projects -->
	<div class="container">
		<h3 id="Projects" style="padding-top: 80px; margin-top: -80px;">Projects</h3><hr>
		
			<h4 id="Project1" style="padding-top: 80px; margin-top: -80px;">Static and Dynamic Gesture Recognition</h4>
				<img src="img/static_gesture.png" height="180px">
				<img src="img/3DCNN.png" height="180px">
				<br>
				<p>We implemented static gesture recognition using a convolutional neural network, obtained an accuracy of 90% on <a target="_blank" href="http://www.idiap.ch/resource/gestures/">Sebastien Marcel Static Hand Posture Database</a> (6 categories). We also extended to use this gesture recognition model to control the Dino Jump game.<br>
					
				We also implemented dynamic gesture recognition using a two-stream 3D convolutional neural network, obtained an accuracy of 91% on <a target="_blank" href="http://lshao.staff.shef.ac.uk/data/SheffieldKinectGesture.htm">Sheffield KInect Gesture (SKIG) Dataset</a> (10 categories).
				</p>
		
			<hr>
			
			<h4 id="Project2" style="padding-top: 80px; margin-top: -80px;">Text and Image Classification</h4>
				<img src="img/text_classify.png" height="180px">
				<img src="img/image_classify.png" height="180px">
				<br>
				<p>We implemented text classification using <i>scikit-learn</i>. Compared the performance of different classifiers (Naive Bayesian, SVM, SGD, Decision Tree, KNN, K-means), achieved 85% accuracy (9 categories).<br>
					
				We also implemented images classification using <i>Keras</i> on a subset of ImageNet, achieved 80% accuracy (19 categories). We designed a CNN which consists of 3 convolutional layers followed by pooling layers and one fully connected layer.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/Machine-Learning--Classification"> <small>[Source Code]</small></a>

			<hr>

			<h4 id="Project3" style="padding-top: 80px; margin-top: -80px;">Face Detection and Recognition</h4>
				<img src="img/face.png" height="180px">
				<br>
				<p>In this project, we detected faces in given images, matched the faces to examples in a given photo gallery and identified the person. Face detection and alignment processes are implemented in Dlib. Face recognition uses a deep learning model that is fine-tuned from <a target="_blank" href="https://arxiv.org/abs/1412.1265"><i>Deeply learned face representations are sparse, selective, and robust</i></a> (DeepID2+).
				</p>

			<hr>

			<h4 id="Project4" style="padding-top: 80px; margin-top: -80px;">Visualizing the Bank Marketing Data Set</h4>
				<img src="img/visualization1.png" height="180px">
				<img src="img/visualization2.png" height="180px">
				<br>
				<p>In this project, I developed a client, server and database system to visualize the <a target="_blank" href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing">Bank Marketing Data Set</a>, with an interactive interface that allows users to customize the visualization.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/Visualization-of-Bank-Marketing-Dataset"> <small>[Source Code]</small></a>

			<hr>

			<h4 id="Project5" style="padding-top: 80px; margin-top: -80px;">Design and Control Robots In Simulation</h4>
				<img src="img/robot1.png" height="180px">
				<img src="img/robot2.png" height="180px">
				<img src="img/robot3.png" height="180px">
				<br>
				<p>In this project, we designed a multi-robot system on Webots where a team of robots are instructed to perform a set of navigation and interaction tasks.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/RobotControlOnWebots"> <small>[Source Code]</small></a>

			<hr>

			<h4 id="Project6" style="padding-top: 80px; margin-top: -80px;">Blend Pictures Seamlessly with Poisson Image Editing
</h4>
				<img src="img/seamless.png" height="180px">
				<br>
				<p>We learnt the paper of <a target="_blank" href="http://www.cs.virginia.edu/~connelly/class/2014/comp_photo/proj2/poisson.pdf">Poisson Image Editing</a> and the source code of the exiting function in OpenCV, then implemented the algorithm using C++ and OpenCV in Visual Studio.
				</p>

			<hr>

			<h4 id="Project7" style="padding-top: 80px; margin-top: -80px;">Mine Sweeper</h4>
				<img src="" height="180px">
				<br>
				<p>We designed the game Mine Sweeper using C++. In our implementation, we provided time counting in game, saving and loading at any time on your turn, including 3-level difficulty with different board size. We also wrote a simple graphical user interface with EasyX.
				</p>

			<hr>

		<br>
	</div><br>

	
	<!-- Honor -->
	<div class="container">
		<h3 id="Honors" style="padding-top: 80px; margin-top: -80px;">Honors and Awards</h3><hr>
		<ul>
			<li><p><b>Outstanding Research Award</b>, Peking University, 2015~2016</p></li>
			<li><p><b>Wu Si Scholarship</b>, Peking University, 2015~2016</p></li>
			<li><p><b>Chang Fei Scholarship</b>, School of EECS, Peking University, 2016~2017</p></li>
		</ul><hr>
		
		<br>		
	</div><br>
	
	
	<!-- Contact -->
	<div class="container">
		<h3 id="Contact" style="padding-top: 80px; margin-top: -80px;">Contact</h3><hr>
		Room 302, Yanyuan33, Peking University, <br>
		Yiheyuan Road #5, Beijing, 100871, China. <br>
		<a href="mailto:zhangluxin@pku.edu.cn">zhangluxin@pku.edu.cn</a><br>
	</div>

	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Peking University 2017</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=nKR2t_3ZegMV5woZHR0IquZryOK5Za-lxAEe19P3uVE&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'>
	</script>
	<br>
</body>

</html>
