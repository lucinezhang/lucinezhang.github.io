<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
</head>


<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
		<a class="navbar-brand" href="#">Luxin Zhang</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
	  		<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Education">Education</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Research">Research</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item dropdown">
					<a class="nav-link dropdown-toggle" href="" id="projects" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects</a>
					<div class="dropdown-menu dropdown-menu-right" aria-labelledby="projects">
						<a class="dropdown-item" href="#Project1">Mine Sweeper</a>
						<a class="dropdown-item" href="#Project2">Yuzhe</a>
						<a class="dropdown-item" href="#Project3">Sth else here</a>
					</div>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Honors">Honors</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 20px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 0px; padding-top: 62px">
				<img class="img-responsive img-rounded" src="img/Luxin.jpeg" alt="" style="max-width: 260px">
			</div>
			<div class="col-md-9">
			<br><br>

			<p>Hi, I'm Luxin Zhang!</br>
			I am a senior undergraduate student at 
			<b><a target="_blank" href="http://www.cis.pku.edu.cn/english_index.htm">Department of Intelligence Science</a></b>, 
			<b><a target="_blank" href="http://eecs.pku.edu.cn/EN/">School of EECS</a></b>, 
			<b><a target="_blank" href="http://english.pku.edu.cn/">Peking University</a></b>.<br>

			I was a visiting student at
			<b><a target="_blank" href="https://www.cs.utexas.edu/">Department of Computer Science</a></b>, 
			<b><a target="_blank" href="https://www.utexas.edu/">The University of Texas at Austin</a></b>, 
			instructed by Professor 
			<b><a target="_blank" href="http://www.cs.utexas.edu/~dana/">Dana Ballard</a></b> 
			during July and August 2017.<br>

			I am currently working in 
			<b><a target="_blank" href="https://www.microsoft.com/en-us/research/group/bdm/">Big Data Mining Group</a></b> at 
			<b><a target="_blank" href="http://www.msra.cn/">Microsoft Research Asia</a></b>, supervised by Dr. 
			<b><a target="_blank" href="https://www.microsoft.com/en-us/research/people/borjekar/">Borje Karlsson</a></b> 
			since September 2017.<br>

			I am interested in Machine Learning, Computer Vision and Artificial Intelligence.<br>
			<br>
			<a href="static/Luxin Zhang.pdf">CV</a> | <a href="https://github.com/lucinezhang">Github</a> | <a href="https://www.linkedin.com/in/luxin-zhang-0264a411a/">LinkedIn</a></p>
			</p>

			</div>
		</div>
	</div><br><br>

	
	<!-- Education -->
	<div class="container">
		<h3 id="Education" style="padding-top: 80px; margin-top: -80px;">Educations</h3><hr>
		<table class="imgtable"><tr><td>
			<img src="img/pku.png" alt="Peking" width="100px" height="100px" />&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td align="left">
				<p><i>Sept. 2014 - Jul. 2018 (Expected)</i> <br>
				<a target="_blank" href="http://www.cis.pku.edu.cn/english_index.htm">Department of Computer Intelligence Science</a>, 
				<b><a target="_blank" href="http://english.pku.edu.cn/">Peking University</a></b><br>
				Bachelor of Science <br>
				<b>Overall GPA: 3.54/4.0 &nbsp; Junior GPA: 3.70/4.0</b>
				</p>
		</td></tr></table>

		<table class="imgtable"><tr><td>
			<img src="img/utaustin.png" alt="UT-Austin" width="100px" height="100px" />&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td align="left">
				<p><i>Jul. 2017 - Sept. 2017</i> <br>
				<a target="_blank" href="https://www.cs.utexas.edu/">School of Computer Science</a>, 
				<b><a target="_blank" href="https://www.utexas.edu/">The University of Texas at Austin</a></b> <br>
				<a target="_blank" href="http://www.cs.utexas.edu/~dana/vrlab/">Vision, Cognition, and Action VR Lab</a> <br>
				Research Assistant <br>
				Supervised by Professor <b>Dana Ballard</b>.
				</p>
		</td></tr></table>
		<hr>
		<br>
		
	</div><br>

	
	<!-- Research -->
	<div class="container">
		<h3 id="Research" style="padding-top: 80px; margin-top: -80px;">Research</h3><hr>
			<h4 style="padding-top: 80px; margin-top: -80px;">Modeling human attention for deep imitation learning</h4>
				<p>Time: <i>Jul. 2017 - Present</i></p>
				<img src="img/seaquest.png" height="180px">
				<img src="img/breakout.png" height="180px">
				<img src="img/gaze_network.png" height="180px">
				<img src="img/action_network.png" height="150px">
				<br>
				<p>When an intelligent agent learns to imitate human visuomotor behaviors, it may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agent's performance. With this motivation, we collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another deep neural network to predict human actions (the policy network). Including the gaze predictions from the gaze network in the policy network significantly improves the action prediction accuracy. We conclude that it is feasible to learn human attention in the given visuomotor tasks, and that combining the learned attention model with imitation learning yields promising results.</p><br>

				<p>My work was to model visual attention from human eye movement data using a deep learning approach. I helped conduct psychophysical experiments that collect high-precision human eye tracking data when playing video games, and designed a three-channel convolution-deconvolution deep neural network, that simultaneously takes game image frames, motion information (optical flow), and image saliency information to predict where the human would allocate visual attention when playing the game. Eventually I obtained a deep neural network model that can predict human visual attention with high precision. Now we are experimenting on using this visual attention model to facilitate the learning process of deep reinforcement learning and deep imitation learning algorithms.
				</p><hr>
				
			<h4 style="padding-top: 80px; margin-top: -80px;">Text Effects Transfer</h4>
				<p>Time: <i>Mar. 2017 - Jun. 2017</i></p>
				<img src="img/text_transfer.png" height="180px">
				<br>
				<p>Text effects transfer is a pretty novel research area. We studied the problem of transferring the text styles from source text image to target text image, that is, given a source stylized image S' and the target text image T, then automatically generates the target stylized image T' with the special e ects as in S'. I tried different image segmentation methods to create a text mask for the stylized image S', like KNN clustering based on pixels' feature vectors and level set segmentation based on shape priors.
				</p><hr>

			<h4 style="padding-top: 80px; margin-top: -80px;">Cultural heritage protection based on virtual reality</h4>
				<p>Time: <i>Mar. 2016 - Jun. 2016</i></p>
				<br>
				<p>I participated in an interdisciplinary project between computer vision and archaeological conservation, in which we attempted to repair the faces of the Buddha of Longmen Grottoes using archived photos and display them on a virtual reality (VR) system for access in a digital museum. I was responsible for designing the VR user interface based on gesture recognition to enable the visitors to instruct the system to display the images, show text descriptions, or start the voice guide. I learned to use both a Hidden Markov Model and a 3D-convolutional neural network to complete the dynamic gesture classification.
				</p><hr>
		
		<br>
		
	</div><br>
	
	
	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">Publications</h3><hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="img/seaquest.png" alt="" height="180px">
			</div>
			<div class="col-md-9">
			<b><font color="black">Learning Attention Model from Human for Visuomotor Tasks</font></b><br>
			<a target="_blank" href="http://lucinezhang.github.io"><u><font color="black">L. Zhang</font></u></a>,
			<a target="_blank" href="http://www.cs.utexas.edu/~zharucs/">R. Zhang</a>, 
			Z. Liu, 
			<a target="_blank" href="https://liberalarts.utexas.edu/cps/faculty/profile.php?id=mmh739">M. Hayhoe</a> 
			and 
			<a target="_blank" href="http://www.cs.utexas.edu/~dana/">D. Ballard</a>
			<br>
			<i><a target="_blank" href="https://aaai.org/Conferences/AAAI-18/aaai18studentcall/">AAAI'18 Student Abstract and Poster Program</a></i>, New Orleans, Louisiana, USA, Feb. 2018<br>
			<a target="_blank" href="static/AAAI-18.pdf"> <small>[PDF]</small></a>
			<a target="_blank" href="https://www.youtube.com/watch?v=-zTX9VFSFME"> <small>[Video]</small></a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="img/gaze_network.png" alt="">
				<img class="img-fluid img-rounded" src="img/action_network.png" alt="">
			</div>
			<div class="col-md-9">
			<b><font color="black">Visual Attention Guided Deep Imitation Learning</font></b>
			<br>
			<a target="_blank" href="http://www.cs.utexas.edu/~zharucs/">R. Zhang</a>, 
			Z. Liu, 
			<a target="_blank" href="http://lucinezhang.github.io"><u><font color="black">L. Zhang</font></u></a>,
			K. Muller, 
			<a target="_blank" href="https://liberalarts.utexas.edu/cps/faculty/profile.php?id=mmh739">M. Hayhoe</a> 
			and 
			<a target="_blank" href="http://www.cs.utexas.edu/~dana/">D. Ballard</a>
			<br>
			spotlight paper, 
			<i><a target="_blank" href="https://sites.google.com/view/ciai2017/home">NIPS'17 Cognitively Informed Arti cial Intelligence Workshop</a></i>, Long Beach, California, USA, Dec. 2017<br>
			<a target="_blank" href="static/2017_CIAI.pdf"> <small>[PDF]</small></a>
			</div>
		</div><hr>
		
		<!--
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="files/vetrack.png" alt="">
			</div>
			<div class="col-md-9">
			<b>VeTrack: Real Time Vehicle Tracking in Uninstrumented Indoor Environments</b><br>
			<a href="http://people.csail.mit.edu/mingmin/">M. Zhao</a>,
			R. Gao,
			T. Ye,
			<a href="http://www.ece.stonybrook.edu/~fanye/index2.html">F. Ye</a>,
			<a href="http://www.idm.pku.edu.cn/staff/wangyizhou/">Y. Wang</a> and
			<a href="http://ceca.pku.edu.cn/en/team.php?action=show&member_id=18">G. Luo</a>
			<br>
			<i><a target="_blank" href="http://sensys.acm.org/2015/">ACM SenSys¡¯15</a></i>, Seoul, South Korea, November 2015<br>
			<a href="papers/vetrack-paper.pdf"> <small>[PDF]</small></a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="files/predictencoding.png" alt="">
			</div>
			<div class="col-md-9">
			<b>Predictive Encoding of Contextual Relationships for Perceptual Inference, Interpolation and Prediction</b><br>
			<a href="http://people.csail.mit.edu/mingmin/">M. Zhao</a>,
			C. Zhuang,
			<a href="http://www.idm.pku.edu.cn/staff/wangyizhou/">Y. Wang</a> and
			<a href="http://www.cnbc.cmu.edu/~tai/">T.S. Lee</a>
			<br>
			<i><a target="_blank" href="http://www.iclr.cc/doku.php?id=iclr2015:main">ICLR¡¯15</a></i>, San Diego, California, May 2015<br>
			<a href="papers/predictencoding-paper.pdf"> <small>[PDF]</small></a>
			</div>
		</div><hr> -->
		
		<br>
	</div><br>
	
	
	<!-- Projects -->
	<div class="container">
		<h3 id="Projects" style="padding-top: 80px; margin-top: -80px;">Projects</h3><hr>
		
			<h4 id="Project1" style="padding-top: 80px; margin-top: -80px;">Mine Sweeper</h4>
				<img style="">
		
			<hr>
			
			<h4 id="Project2" style="padding-top: 80px; margin-top: -80px;">Yuzhe</h4>
				<img style="">
		<br>

	</div><br>

	
	<!-- Honor -->
	<div class="container">
		<h3 id="Honors" style="padding-top: 80px; margin-top: -80px;">Honors and Awards</h3><hr>
		<ul>
			<li><p><b>Outstanding Research Award</b>, Peking University, 2015~2016</p></li>
			<li><p><b>Wu Si Scholarship</b>, Peking University, 2015~2016</p></li>
			<li><p><b>Chang Fei Scholarship</b>, School of EECS, Peking University, 2016~2017</p></li>
		</ul><hr>
		
		<br>		
	</div><br>
	
	
	<!-- Contact -->
	<div class="container">
		<h3 id="Contact" style="padding-top: 80px; margin-top: -80px;">Contact</h3><hr>
		Room 302, Yanyuan33, Peking University, <br>
		Yiheyuan Road #5, Beijing, 100871, China. <br>
		<a href="mailto:zhangluxin@pku.edu.cn">zhangluxin@pku.edu.cn</a><br>
	</div>

	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Peking University 2017</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
